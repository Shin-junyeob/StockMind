# code/crawling/crawling.py
# -*- coding: utf-8 -*-
from __future__ import annotations

import os
import gc
import time
import random
import datetime as dt
from typing import List, Dict, Optional

import pandas as pd
from bs4 import BeautifulSoup

# Selenium
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import WebDriverException, TimeoutException

# í”„ë¡œì íŠ¸ ì„¤ì •/í•¨ìˆ˜
from .settings import RAW_DIR, UA_LIST, STOP_LOOKBACK_DAYS, STOP_TOPN
from .article_fetcher import fetch_articles_http

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# (ì„ íƒ) ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°: psutilì´ ìˆìœ¼ë©´ ì‹¤í–‰ í›„ ìš”ì•½ ì¶œë ¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    import psutil
except Exception:
    psutil = None


class ResourceMonitor:
    """ì‹¤í–‰ ì‹œê°„/CPU/ë©”ëª¨ë¦¬(í”¼í¬) ìš”ì•½ìš©. psutil ì—†ìœ¼ë©´ No-Op."""
    def __init__(self, label: str = "run"):
        self.label = label
        self.enabled = psutil is not None
        self._proc = psutil.Process(os.getpid()) if self.enabled else None
        self._start = None
        self._peak_rss = 0
        self._peak_swap = 0
        self._peak_ram_pct = 0.0
        self._cpu_samples = []
        self._last_cpu_time = None

    def __enter__(self):
        self._start = time.perf_counter()
        if self.enabled:
            # ì²« ìƒ˜í”Œ
            mem = self._proc.memory_info().rss
            self._peak_rss = max(self._peak_rss, mem)
            vm = psutil.virtual_memory()
            self._peak_ram_pct = max(self._peak_ram_pct, vm.percent)
            sw = psutil.swap_memory()
            self._peak_swap = max(self._peak_swap, sw.used)
            self._last_cpu_time = self._proc.cpu_times()
        return self

    def sample(self):
        if not self.enabled:
            return
        try:
            mem = self._proc.memory_info().rss
            self._peak_rss = max(self._peak_rss, mem)
            vm = psutil.virtual_memory()
            self._peak_ram_pct = max(self._peak_ram_pct, vm.percent)
            sw = psutil.swap_memory()
            self._peak_swap = max(self._peak_swap, sw.used)

            # ê°„ë‹¨í•œ CPU ì‚¬ìš©ë¥  ìƒ˜í”Œ(í”„ë¡œì„¸ìŠ¤ ê¸°ì¤€ ì¶”ì •)
            now_cpu = self._proc.cpu_times()
            if self._last_cpu_time:
                delta_user = now_cpu.user - self._last_cpu_time.user
                delta_sys = now_cpu.system - self._last_cpu_time.system
                self._cpu_samples.append(max(0.0, delta_user + delta_sys))
            self._last_cpu_time = now_cpu
        except Exception:
            pass

    def tick(self):
        # í•„ìš” ì‹œ ì¤‘ê°„ì¤‘ê°„ í˜¸ì¶œ(í‹°ì»¤ ì¢…ë£Œ ì‹œ ë“±)
        self.sample()

    def __exit__(self, exc_type, exc, tb):
        elapsed = time.perf_counter() - self._start if self._start else 0.0
        # CPU í‰ê· (ëŒ€ëµì ì¸ ì¶”ì •; ìƒ˜í”Œ ê°„ ê°„ê²©ì„ ê³ ë ¤í•˜ì§€ ì•Šì€ ëˆ„ì  ì‹œê°„ ê¸°ë°˜)
        cpu_total = sum(self._cpu_samples)
        cpu_avg = (cpu_total / elapsed) * 100.0 if self.enabled and elapsed > 0 else 0.0

        def _mb(x: int) -> float:
            return round(x / (1024 * 1024), 1)

        print("\nâ”€â”€â”€â”€ Resource Summary [{}] â”€â”€â”€â”€".format(self.label))
        print(f"â±  Elapsed: {elapsed:.2f} sec")
        if self.enabled:
            print(f"ğŸ§   Peak RSS: {_mb(self._peak_rss)} MB")
            print(f"ğŸ’¾  Peak Swap Used: {_mb(self._peak_swap)} MB")
            print(f"ğŸ“ˆ  Peak RAM Usage (system): {self._peak_ram_pct:.1f}%")
            print(f"âš™ï¸  CPU (rough avg): {cpu_avg:.1f}%")
        else:
            print("â„¹ï¸  psutil ë¯¸ì„¤ì¹˜ â†’ ë©”ëª¨ë¦¬/CPU ìš”ì•½ ìƒëµë¨ (pip install psutil ê¶Œì¥)")
        print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìƒìˆ˜/ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATE_FMT = "%Y-%m-%d"
YF_BASE_NEWS_URL = "https://finance.yahoo.com/quote/{ticker}/news?p={ticker}"


def _norm_url(u: str | None) -> str:
    return str(u).strip() if u is not None else ""


def _last_date_dir(base_dir: str, ticker: str) -> Optional[str]:
    """
    data/raw/{ticker}/ ì•„ë˜ YYYY-MM-DD í´ë” ì¤‘ ê°€ì¥ ìµœê·¼ ë‚ ì§œë¥¼ ë°˜í™˜. ì—†ìœ¼ë©´ None.
    """
    tdir = os.path.join(base_dir, ticker)
    if not os.path.exists(tdir):
        return None
    dates = []
    for name in os.listdir(tdir):
        p = os.path.join(tdir, name)
        if not os.path.isdir(p):
            continue
        try:
            dt.datetime.strptime(name, DATE_FMT)
            dates.append(name)
        except ValueError:
            pass
    return max(dates) if dates else None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# stop set: url ì»¬ëŸ¼ë§Œ ê°€ë³ê²Œ ë¶€ë¶„ ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _load_urls_head(save_dir: str, top_n: int) -> list[str]:
    pq = os.path.join(save_dir, "news.parquet")
    csv = os.path.join(save_dir, "news.csv")
    try:
        if os.path.exists(pq):
            df = pd.read_parquet(pq, columns=["url"])
            return df["url"].dropna().astype(str).str.strip().head(top_n).tolist()
        if os.path.exists(csv):
            df = pd.read_csv(csv, usecols=["url"], nrows=top_n * 2)
            return df["url"].dropna().astype(str).str.strip().head(top_n).tolist()
    except Exception:
        return []
    return []


def _load_recent_urls_multi(out_dir: str, ticker: str, lookback_days: int, top_n: int) -> list[str]:
    """
    ìµœê·¼ lookback_days ë²”ìœ„ì˜ ë‚ ì§œ í´ë”ì—ì„œ ê° ë‚ ì§œì˜ ìƒë‹¨ top_nê°œ URLë§Œ ëª¨ìŒ.
    (íŒŒì¼ ì „ì²´ë¥¼ ë¡œë“œí•˜ì§€ ì•Šë„ë¡ url ì»¬ëŸ¼ë§Œ ë¶€ë¶„ ë¡œë“œ)
    """
    base = os.path.join(out_dir, ticker)
    if not os.path.exists(base):
        return []

    today = dt.date.today()
    items = []
    for name in os.listdir(base):
        p = os.path.join(base, name)
        if not os.path.isdir(p):
            continue
        try:
            d = dt.datetime.strptime(name, DATE_FMT).date()
        except ValueError:
            continue
        if (today - d).days <= lookback_days:
            items.append(d)
    if not items:
        return []

    items.sort(reverse=True)
    seen, st = [], set()
    for d in items:
        urls = _load_urls_head(os.path.join(base, d.strftime(DATE_FMT)), top_n=top_n)
        for u in urls:
            if u not in st:
                st.add(u)
                seen.append(u)
                if len(seen) >= top_n:   # ì¶©ë¶„í•˜ë©´ ì¦‰ì‹œ ì¢…ë£Œ
                    return seen
    return seen


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Selenium ë“œë¼ì´ë²„(ë¦¬ì†ŒìŠ¤ ìµœì†Œí™” ì„¤ì •)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _make_driver(user_agent: str | None = None):
    opts = Options()
    opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--window-size=1920,1080")

    if user_agent:
        opts.add_argument(f"--user-agent={user_agent}")

    use_remote = os.getenv("USE_REMOTE_WEBDRIVER", "false").lower() == "true"

    if use_remote:
        remote_url = os.getenv("SELENIUM_REMOTE_URL", "http://selenium:4444")
        return webdriver.Remote(command_executor=remote_url, options=opts)

    # ë¡œì»¬ ì‹¤í–‰ìš© (ì»¨í…Œì´ë„ˆ ì•ˆì— í¬ë¡¬ ìˆì„ ë•Œë§Œ)
    service = Service()
    return webdriver.Chrome(service=service, options=opts)


def _driver_get_with_retry(driver, url: str, tries: int = 2, sleep_sec: float = 2.0):
    last_err: Optional[Exception] = None
    for _ in range(tries):
        try:
            driver.get(url)
            return
        except (TimeoutException, WebDriverException) as e:
            last_err = e
        time.sleep(sleep_sec)
    raise last_err if last_err else TimeoutException(f"driver.get({url}) failed")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë§í¬ ìˆ˜ì§‘ (ë¦¬ì†ŒìŠ¤ ìµœì†Œí™”): ìŠ¤í¬ë¡¤ â†’ ìµœì¢… page_source í•œ ë²ˆë§Œ íŒŒì‹±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def collect_yahoo_links_incremental(
    ticker: str,
    max_scroll: int,
    last_date: Optional[str],
    user_agent: str,
) -> List[Dict]:
    """
    ë¦¬ì†ŒìŠ¤ ìµœì†Œí™” ë²„ì „:
    - ìŠ¤í¬ë¡¤ë§Œ ë°˜ë³µ
    - ë§ˆì§€ë§‰ì— ë‹¨ 1íšŒ BeautifulSoup íŒŒì‹±
    - last_dateë³´ë‹¤ ê³¼ê±° ì¹´ë“œë“¤ì€ ëª©ë¡ì—ì„œ ì œì™¸(ìŠ¤í¬ë¡¤ ì¤‘ë‹¨ì€ ì˜ë¯¸ ì ìŒ)
    ë°˜í™˜: [{'url': str, 'date_guess': 'YYYY-MM-DD' or None}, ...]
    """
    url = YF_BASE_NEWS_URL.format(ticker=ticker)
    driver = _make_driver(user_agent=user_agent)
    html = ""
    try:
        _driver_get_with_retry(driver, url, tries=int(os.environ.get("YF_GET_RETRIES", "2")))
        time.sleep(1.8)

        for _ in range(max_scroll):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1.2)

        html = driver.page_source  # ìµœì¢…ë³¸ë§Œ ì‚¬ìš©
    finally:
        try:
            driver.quit()
        except Exception:
            pass

    soup = BeautifulSoup(html, "html.parser")
    cards = soup.select('section[data-testid="storyitem"]')
    results, seen = [], set()

    for card in cards:
        a = card.find("a")
        href = a.get("href") if a else None
        if not href:
            continue
        link = href if href.startswith("http") else ("https://finance.yahoo.com" + href)
        if link in seen:
            continue
        seen.add(link)

        # ë‚ ì§œ ì¶”ì •
        t = card.select_one("time[datetime]")
        guess = None
        if t and t.has_attr("datetime"):
            try:
                guess = t["datetime"].split("T")[0]
            except Exception:
                pass

        # ìŠ¤í¬ë¡¤ ì™„ë£Œ í›„ í•„í„°ë§(ëª©ë¡ ì¶•ì†Œ)
        if last_date and guess and guess < last_date:
            continue

        results.append({"url": link, "date_guess": guess})

    # ë©”ëª¨ë¦¬ í•´ì œ ìœ ë„
    del soup, cards, html
    gc.collect()
    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‚ ì§œë³„ ì €ì¥(ê¸°ì¡´ urlë§Œ ë¶€ë¶„ ë¡œë“œ â†’ ì‹ ê·œë§Œ ë³‘í•© â†’ ì €ì¥)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _save_by_article_date(rows: List[Dict], ticker: str, out_dir: str) -> None:
    if not rows:
        return

    df_all = pd.DataFrame(rows)
    for c in ["date", "url", "title", "content", "status_code", "fetched_at", "run_id", "ticker"]:
        if c not in df_all.columns:
            df_all[c] = ""

    for date_str, df_day in df_all.groupby("date"):
        save_dir = os.path.join(out_dir, ticker, str(date_str))
        os.makedirs(save_dir, exist_ok=True)

        pq = os.path.join(save_dir, "news.parquet")
        csv = os.path.join(save_dir, "news.csv")

        # ê¸°ì¡´ URLë§Œ ê°€ë³ê²Œ ì½ì–´ ì¤‘ë³µ ì œê±°
        existed = set()
        try:
            if os.path.exists(pq):
                existed = set(pd.read_parquet(pq, columns=["url"])["url"].astype(str).map(_norm_url))
            elif os.path.exists(csv):
                existed = set(pd.read_csv(csv, usecols=["url"])["url"].astype(str).map(_norm_url))
        except Exception:
            existed = set()

        df_day = df_day.copy()
        df_day["url"] = df_day["url"].astype(str).map(_norm_url)
        df_new = df_day[~df_day["url"].isin(existed)]

        if df_new.empty:
            continue

        # ìµœì¢… ì €ì¥ ì§ì „ì—ë§Œ ì „ì²´ ë¡œë“œ 1íšŒ
        if os.path.exists(pq):
            old = pd.read_parquet(pq)
        elif os.path.exists(csv):
            old = pd.read_csv(csv)
        else:
            old = pd.DataFrame()

        merged = pd.concat([old, df_new], ignore_index=True, sort=False) if not old.empty else df_new
        merged.drop_duplicates(subset=["url"], keep="first", inplace=True)

        try:
            merged.to_parquet(pq, index=False)
            print(f"[{ticker}] ì €ì¥ ì™„ë£Œ: {pq} (rows={len(merged)})")
        except Exception:
            merged.to_csv(csv, index=False)
            print(f"[{ticker}] Parquet ë¯¸ì§€ì›ìœ¼ë¡œ CSV ì €ì¥: {csv}")

        # ë©”ëª¨ë¦¬ í•´ì œ
        del old, merged, df_new, df_day
        gc.collect()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ íŒŒì´í”„ë¼ì¸ (í´ë°± Selenium OFF + í‹°ì»¤ë§ˆë‹¤ GC + ë¦¬ì†ŒìŠ¤ ìš”ì•½)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_yahoo_pipeline(
    tickers: List[str],
    run_date: Optional[str] = None,       # ë¡œê·¸ í‘œê¸°(ì €ì¥ì€ ê¸°ì‚¬ date)
    run_id: str = "local-dev",
    out_dir: str = RAW_DIR,
    max_scroll: int = 20,
    max_articles_per_ticker: int = 200,
    requests_ua_mode: str = "round_robin",
) -> None:
    """
    ë¦¬ì†ŒìŠ¤ ìµœì†Œí™” ì›ì¹™:
    - í‹°ì»¤ë‹¹ Selenium 1íšŒ(ìŠ¤í¬ë¡¤ë§Œ), Soup 1íšŒ íŒŒì‹±
    - ë³¸ë¬¸ ìˆ˜ì§‘ì€ requestsë§Œ ì‚¬ìš©(í´ë°± Selenium OFF)
    - ê¸°ì¡´ íŒŒì¼ì€ urlë§Œ ë¶€ë¶„ ë¡œë“œí•´ ì¤‘ë³µ ì œê±°
    - í‹°ì»¤ ê²½ê³„ë§ˆë‹¤ GC ê°•ì œ í˜¸ì¶œ
    - ì‹¤í–‰ í›„ ë¦¬ì†ŒìŠ¤ ìš”ì•½ ì¶œë ¥(psutil ìˆìœ¼ë©´)
    """
    run_date = run_date or dt.datetime.now().strftime(DATE_FMT)

    with ResourceMonitor(label="crawling") as mon:
        for ticker in tickers:
            last_date = _last_date_dir(out_dir, ticker)  # "YYYY-MM-DD" or None
            stop_urls = _load_recent_urls_multi(
                out_dir=out_dir,
                ticker=ticker,
                lookback_days=STOP_LOOKBACK_DAYS,
                top_n=STOP_TOPN,
            )

            ua = random.choice(UA_LIST) if UA_LIST else ""
            items = collect_yahoo_links_incremental(
                ticker=ticker,
                max_scroll=max_scroll,
                last_date=last_date,
                user_agent=ua,
            )
            if not items:
                print(f"[{ticker}] ìƒˆ ë§í¬ ì—†ìŒ. (run_date={run_date})")
                mon.tick(); gc.collect()
                continue

            # last_date ê¸°ë°˜ 1ì°¨ í•„í„°
            if last_date:
                items = [it for it in items if (it["date_guess"] is None) or (it["date_guess"] >= last_date)]
                if not items:
                    mon.tick(); gc.collect()
                    continue

            # stop set ì¤‘ë³µ ì œê±°
            if stop_urls:
                stop_set = set(map(_norm_url, stop_urls))
                items = [it for it in items if _norm_url(it["url"]) not in stop_set]
                if not items:
                    mon.tick(); gc.collect()
                    continue

            # ìƒí•œ ì»·
            items = items[:max_articles_per_ticker]
            urls = [it["url"] for it in items]

            # ë³¸ë¬¸ ìˆ˜ì§‘: requestsë§Œ (í´ë°± Selenium OFF)
            articles = fetch_articles_http(
                urls=urls,
                ua_mode=requests_ua_mode,
                delay_range=(0.6, 1.1),
                min_len_for_ok=120,
                enable_selenium_fallback=False,
            )
            if not articles:
                mon.tick(); gc.collect()
                continue

            # ë³¸ë¬¸ date í™•ì • í›„ last_date ê¸°ì¤€ ìµœì¢… í•„í„°
            if last_date:
                articles = [a for a in articles if a.get("date") and a["date"] >= last_date]
                if not articles:
                    mon.tick(); gc.collect()
                    continue

            now_iso = dt.datetime.now().isoformat(timespec="seconds")
            rows: List[Dict] = [{
                "ticker": ticker,
                "url": a.get("url"),
                "title": a.get("title", ""),
                "date": a.get("date"),
                "content": a.get("content", ""),
                "status_code": a.get("status_code"),
                "fetched_at": now_iso,
                "run_id": run_id,
            } for a in articles]

            _save_by_article_date(rows, ticker=ticker, out_dir=out_dir)

            # í‹°ì»¤ ì¢…ë£Œ ì‹œ ìƒ˜í”Œ/GC
            mon.tick()
            del items, urls, articles, rows, stop_urls
            gc.collect()

